{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b9b5ef",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This project is aimed at extracting the data of hadoop contributors from github api and storing it into mysql database.  \n",
    "\n",
    "### Variables extracted\n",
    "- Name\n",
    "- Id\n",
    "- Location\n",
    "- Hiring status\n",
    "- Bio\n",
    "- Following on github\n",
    "- Contact information\n",
    "- Profile URL\n",
    "\n",
    "### Business Outcome:\n",
    "Data scraped from the internet can be a valuable resource for statistical analysis and research purposes. By storing the data of all contributors on github, a company can easily access the details of potential employees. This information can be used by recruitment team to filter candidates who are hireable and has been contributing to the hadoop space. \n",
    "Instead of going through the profile of each and every individual, the sql database can be queried to access the needed information. \n",
    "\n",
    "### Tools and Technologies Used\n",
    "- MySQL\n",
    "- Beautiful soup\n",
    "\n",
    "\n",
    "\n",
    "Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15508f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import warnings\n",
    "import requests\n",
    "import json\n",
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603cc926",
   "metadata": {},
   "source": [
    "Urban dictionary is not a website but it is providing a json object in return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c3f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.github.com/repos/apache/hadoop/contributors\";\n",
    "header={''}\n",
    "\n",
    "page = requests.get(url,headers=header,params={'per_page': 100},auth=('username','token'))\n",
    "doc = BeautifulSoup(page.content, 'html.parser')\n",
    "json_dict = json.loads(str(doc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a89179f",
   "metadata": {},
   "source": [
    "Fetching profile URLs of contributor to store in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f07670",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_url=[]\n",
    "\n",
    "try:   \n",
    "    for i in range(0,100):\n",
    "        profile_url.append((json_dict[i]['url']))\n",
    "except:\n",
    "    print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1c2ce",
   "metadata": {},
   "source": [
    "Fecthing the cotributor details from the profile URLs saved in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "308e55e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_=[]\n",
    "login_=[]\n",
    "location_=[]\n",
    "email_=[]\n",
    "hireable_=[]\n",
    "bio_=[]\n",
    "twitter_username_=[]\n",
    "public_repos_=[]\n",
    "public_gists_=[]\n",
    "followers_=[]\n",
    "following_=[]\n",
    "created_at_=[]\n",
    "\n",
    "for i in range(0,100):\n",
    "        page = requests.get(profile_url[i],headers=header,auth=('username','auth_token'))\n",
    "        doc = BeautifulSoup(page.content, 'html.parser')\n",
    "        json_dict = json.loads(str(doc))\n",
    "        \n",
    "        user_id_.append(json_dict[\"id\"])\n",
    "        login_.append(json_dict['login'])\n",
    "        location_.append(json_dict[\"location\"])\n",
    "        email_.append(json_dict[\"email\"])\n",
    "        hireable_.append(json_dict[\"hireable\"]) \n",
    "        bio_.append(json_dict[\"bio\"])\n",
    "        twitter_username_.append(json_dict[\"twitter_username\"])\n",
    "        public_repos_.append(json_dict[\"public_repos\"])\n",
    "        public_gists_.append(json_dict[\"public_gists\"])\n",
    "        followers_.append(json_dict[\"followers\"])\n",
    "        following_.append(json_dict[\"following\"])\n",
    "        if json_dict[\"created_at\"] is not None:\n",
    "            json_dict[\"created_at\"] = datetime.strptime(json_dict[\"created_at\"],\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            created_at_.append(json_dict[\"created_at\"])\n",
    "        else: \n",
    "            created_at_.append(json_dict[\"created_at\"])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d433109",
   "metadata": {},
   "source": [
    "Function to create a mysql database and a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1ad6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sql_table(SQL_TABLE_URBAN, SQL_TABLE_URBAN_DEF):\n",
    "    try:\n",
    "\n",
    "        #connect to server\n",
    "        conn = pymysql.connect(host='localhost', user = '', password = ' ')\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        query = \"CREATE DATABASE IF NOT EXISTS \" + SQL_DB\n",
    "        print(query)\n",
    "        cursor.execute(query);\n",
    "\n",
    "        query = \"CREATE TABLE IF NOT EXISTS \" + SQL_DB + \".\" + SQL_TABLE_URBAN + \" \" + SQL_TABLE_URBAN_DEF + \";\";\n",
    "        print(query)\n",
    "        cursor.execute(query);\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return\n",
    "\n",
    "    except IOError as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f322223",
   "metadata": {},
   "source": [
    "Specify the name and structure of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e576babf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE DATABASE IF NOT EXISTS DDR_Github\n",
      "CREATE TABLE IF NOT EXISTS DDR_Github.userprofile (ind INT NOT NULL AUTO_INCREMENT PRIMARY KEY,userid INT ,login VARCHAR(50),location VARCHAR(100),email VARCHAR(30),hireable BOOLEAN,bio VARCHAR(1000),twitter_username VARCHAR(30),public_repos INT,public_gists INT,followers INT,following INT,created_at DATETIME);\n"
     ]
    }
   ],
   "source": [
    "SQL_DB = \"DDR_Github\"\n",
    "TERM = \"blog\"\n",
    "\n",
    "SQL_TABLE_URBAN = \"userprofile\"\n",
    "SQL_TABLE_URBAN_DEF = \"(\" + \\\n",
    "        \"ind INT NOT NULL AUTO_INCREMENT PRIMARY KEY\" + \\\n",
    "        \",userid INT \" + \\\n",
    "        \",login VARCHAR(50)\" + \\\n",
    "        \",location VARCHAR(100)\" + \\\n",
    "        \",email VARCHAR(30)\" + \\\n",
    "        \",hireable BOOLEAN\" + \\\n",
    "        \",bio VARCHAR(1000)\" + \\\n",
    "        \",twitter_username VARCHAR(30)\" + \\\n",
    "        \",public_repos INT\" + \\\n",
    "        \",public_gists INT\" + \\\n",
    "        \",followers INT\" + \\\n",
    "        \",following INT\" + \\\n",
    "        \",created_at DATETIME\" + \\\n",
    "        \")\"\n",
    "\n",
    "create_sql_table(SQL_TABLE_URBAN,SQL_TABLE_URBAN_DEF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7854ffbd",
   "metadata": {},
   "source": [
    "Inserting rows in the table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c8ff4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #connect to server\n",
    "    conn = pymysql.connect(host='localhost', user = '', password = '', database = '')\n",
    "    cursor = conn.cursor()   \n",
    "        \n",
    "        \n",
    "    #standard version\n",
    "#     prepared_stmt = \"INSERT INTO \" + SQL_TABLE_URBAN + \"(userid,login,location,email,hireable,bio, twitter_username,public_repos,public_gists,followers,following ,created_at ) values ('{}', '{}', '{}', '{}', '{}', '{}', '{}', '{}', '{}', '{}', '{}', '{}');\" \n",
    "    \n",
    "    #parametrized version\n",
    "    parameterized_stmt = \"INSERT INTO \" + SQL_TABLE_URBAN + \"(userid,login,location,email,hireable,bio, twitter_username,public_repos,public_gists,followers,following ,created_at ) values (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ;\"\n",
    "    \n",
    "    for i in range(0,100):        \n",
    "        user_id=user_id_[i]\n",
    "        login=login_[i]\n",
    "        location=location_[i]\n",
    "        email=email_[i]\n",
    "        hireable=hireable_[i]\n",
    "        bio=bio_[i]\n",
    "        twitter_username=twitter_username_[i]\n",
    "        public_repos=public_repos_[i]\n",
    "        public_gists=public_gists_[i]\n",
    "        followers=followers_[i]\n",
    "        following=following_[i]\n",
    "        created_at=created_at_[i]\n",
    "        \n",
    "#         stmt = prepared_stmt.format(user_id, login, location, email, ((hireable) if hireable else 0), codecs.escape_encode(str.encode(\"'{}'\".format(bio) if bio else \"NULL\"))[0].decode(), twitter_username, public_repos, \n",
    "#                                     public_gists, followers, following, created_at )\n",
    "        \n",
    "\n",
    "        \n",
    "#         cursor.execute(stmt)\n",
    "        # parameterized version\n",
    "        cursor.execute(parameterized_stmt, (user_id, login, location, email, ((hireable) if hireable else 0), codecs.escape_encode(str.encode(\"'{}'\".format(bio) if bio else \"NULL\"))[0].decode(), twitter_username, public_repos, public_gists, followers, following, created_at))\n",
    "        \n",
    "        \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "except IOError as e:\n",
    "    print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f7b36c",
   "metadata": {},
   "source": [
    "For userid I chose integer as this is an integer. Login is a unique username that gets created with github. This can be a combination of string and integers so varchar is the right choice for it. \n",
    "For location and email, it is a mix of strings and numerals. Special characters such as #, @ can also be present so choosing varchar is the right choice. Hireable is a boolean True or False so boolean data type is used for it. \n",
    "Twitter username is again unique as github login which can be a mix of strings and numbers so varchar datatype is chosen for it. \n",
    "Public repos, public gists,followers and following are numbers hence integer data type is used. \n",
    "For created at, it is a datetime so datetime datatype is used to store the time and date of creation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba09373",
   "metadata": {},
   "source": [
    "Indexing the table for easier and faster access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed977535",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_DB = \"DDR_Github\"\n",
    "TERM = \"blog\"\n",
    "SQL_TABLE_URBAN = \"userprofile\"\n",
    "\n",
    "try:\n",
    "    #connect to server\n",
    "    conn = pymysql.connect(host='localhost', user = '', password = '', database = '')\n",
    "    cursor = conn.cursor()    \n",
    "        \n",
    "    #standard version\n",
    "    prepared_stmt1 = \"CREATE INDEX indexing_login ON \"+ SQL_DB +\".\"+ SQL_TABLE_URBAN +\"(login);\"\n",
    "    prepared_stmt2 = \"CREATE INDEX ind_loc ON \"+ SQL_DB +\".\"+ SQL_TABLE_URBAN +\"(location);\"\n",
    "    prepared_stmt3 = \"CREATE INDEX ind_hire ON \"+ SQL_DB +\".\"+ SQL_TABLE_URBAN +\"(hireable);\"\n",
    "      \n",
    "    cursor.execute(prepared_stmt1)\n",
    "    cursor.execute(prepared_stmt2)\n",
    "    cursor.execute(prepared_stmt3)     \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "except IOError as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a6b774",
   "metadata": {},
   "source": [
    "I went with indexing as index creastes a way for faster lookup. The search doesn't have to be done on each and every row in the data if there is index. Algorithms such as binary search, merge sort takes into account the index making it easier to divide and conquer. SQL uses binary search which makes the effort to look for elements significantly small. Hashcodes are used for indexing string objects. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
